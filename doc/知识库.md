### LangChain.dart 实现知识库（RAG）的完整指南

LangChain.dart（包名：`langchain`）是 Python LangChain 的 Dart 端口，非常适合在 Flutter/Dart 中构建 **Retrieval-Augmented Generation (RAG)** 知识库应用。RAG 的核心是：将自定义文档加载、切分、嵌入向量、存储到向量数据库，然后在查询时检索相关内容注入到 LLM 提示中，避免幻觉并基于私有知识回答。

#### 1. 安装依赖
在 `pubspec.yaml` 中添加：
```yaml
dependencies:
  langchain: ^latest  # 核心包
  langchain_openai: ^latest  # 如果用 OpenAI/Gemini 等
  langchain_ollama: ^latest  # 如果用本地 Ollama（如 Gemma）
  langchain_chroma: ^latest  # 向量存储（本地 Chroma）
  # 其他可选：langchain_pinecone（云端）、langchain_objectbox 等
```

运行 `flutter pub get`。

#### 2. RAG 核心步骤
RAG 流程分为 **索引（Indexing）** 和 **查询（Retrieval + Generation）** 两部分。

##### 步骤1：加载和切分文档（Document Loaders + Text Splitters）
```dart
import 'package:langchain/langchain.dart';
import 'package:langchain_community/langchain_community.dart';  // loaders 等

final loader = TextLoader('path/to/your_knowledge.md');  // 支持 PDF、Web 等多种 loader
final documents = await loader.load();

final textSplitter = RecursiveCharacterTextSplitter(
  chunkSize: 1000,
  chunkOverlap: 200,
);
final chunks = textSplitter.splitDocuments(documents);
```

##### 步骤2：生成嵌入（Embeddings）
选择嵌入模型（推荐本地或云端）：
```dart
// 本地 Ollama（需先运行 ollama run nomic-embed-text）可定义选择其他模型
final embeddings = OllamaEmbeddings(model: 'qwen3-embedding:latest');
```

##### 步骤3：创建向量存储（Vector Store）
推荐本地：Chroma（持久化）、MemoryVectorStore（内存测试用）、ObjectBox（移动端高效）。

示例用 Chroma（本地完全离线）：
```dart
import 'package:langchain_chroma/langchain_chroma.dart';

final vectorStore = Chroma(
  embeddings: embeddings,
  collectionName: 'my_knowledge_base',  // 可持久化到磁盘
);

// 索引：添加切分后的 chunks
await vectorStore.addDocuments(documents: chunks);
```

其他选项：
- MemoryVectorStore：简单内存存储，适合原型。
- Pinecone：云端生产用。

##### 步骤4：构建 Retriever 和 RAG Chain
```dart
final retriever = vectorStore.asRetriever(k: 4);  // 检索 top 4 相关 chunks

// LLM（例如本地 Gemma 2 via Ollama）
final llm = ChatOllama(model: 'gemma2:2b');  // ollama run gemma2:2b

// Prompt 模板（注入检索上下文）
const promptTemplate = '''
你是一个helpful助手，使用以下上下文回答问题。如果不知道，就说不知道。

上下文：{context}

问题：{question}
回答：
''';

final prompt = PromptTemplate.fromTemplate(promptTemplate);

// RAG Chain：检索 -> 格式化 -> 生成
final ragChain = Runnable.fromMap({
  'context': retriever.pipe((docs) => docs.map((d) => d.pageContent).join('\n\n')),
  'question': Runnable.passthrough(),
}).pipe(prompt).pipe(llm).pipe(const StringOutputParser());
```

##### 步骤5：查询知识库
```dart
final result = await ragChain.invoke({'question': '你的问题，例如：氢能源的优势是什么？'});
print(result);  // 输出基于知识库的回答
```

#### 完整本地 RAG 示例（基于 Ollama + Chroma）

- 命名知识库，设置基本参数。
- 上传知识文件（如 md、epub）。
- 矢量化用 OllamaEmbeddings + Chroma 存储。
- 结合 chat 生成。
